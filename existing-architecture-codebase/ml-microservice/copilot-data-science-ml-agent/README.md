# Data Science & ML Chatbot with Automatic Insights

A comprehensive multi-agent chatbot system that analyzes datasets, research papers, and provides expert-level data science and machine learning guidance **with automatic insight generation and evidence-based analysis**. Built using Google's ADK (Agent Development Kit) framework.

## ğŸŒŸ Features

### Multi-Agent Architecture
- **Main Orchestrator Agent**: Coordinates tasks and routes requests to specialized sub-agents
- **CSV Analyzer** (Enhanced): Loads and analyzes tabular data with **automatic EDA and insights**
- **Research Paper Analyzer**: Extracts and analyzes information from PDF research papers
- **Statistical Analyzer**: Performs rigorous statistical tests and hypothesis testing
- **ML Analyzer**: Provides machine learning recommendations and guidance

### ğŸ†• NEW: Automatic Insights & EDA
âœ… **Comprehensive EDA**
- Automatic insight generation across 6 categories
- Data quality scoring (0-100)
- Pattern and anomaly detection
- Evidence-based claims
- Actionable recommendations with priorities
- Data storytelling and narratives

âœ… **Insight Categories**
- **Data Quality**: Missing values, duplicates, constant columns
- **Distributions**: Skewness, normality, zero-inflation
- **Relationships**: Correlations, multicollinearity
- **Patterns**: Trends, dominant categories, high cardinality
- **Anomalies**: Outliers, extreme values
- **Summary**: Key findings and recommendations

### Core Capabilities
âœ… **Data Analysis**
- Load and inspect CSV files
- **Automatic comprehensive EDA** (NEW!)
- **Generate data-driven insights** (NEW!)
- **Evidence-based analysis** (NEW!)
- Descriptive statistics for all data types
- Correlation analysis
- Outlier detection
- Data quality assessment

âœ… **Statistical Analysis**
- T-tests (independent, paired)
- Chi-square tests
- ANOVA
- Normality testing
- Confidence intervals
- Advanced hypothesis testing

âœ… **Research Paper Analysis**
- PDF text extraction
- Key findings extraction
- Methodology identification
- Statistical information extraction
- Paper summarization

âœ… **Machine Learning Guidance**
- Algorithm recommendations
- Feature engineering suggestions
- ML readiness assessment
- Model evaluation strategies

âœ… **Visualization**
- Histograms
- Box plots
- Correlation heatmaps
- Scatter plots
- Line plots
- Bar charts
- Distribution plots
- Time series plots

âœ… **Session Management**
- Persistent session storage
- Conversation history tracking
- Analysis results caching
- File upload management

## ğŸ—ï¸ Architecture

```
data_science_agent/
â”œâ”€â”€ database/              # Session and state management
â”‚   â”œâ”€â”€ models.py         # SQLAlchemy models
â”‚   â””â”€â”€ db_manager.py     # Database operations
â”œâ”€â”€ shared_libraries/     # Common utilities
â”‚   â”œâ”€â”€ data_loader.py    # Data loading utilities
â”‚   â”œâ”€â”€ statistics_helper.py  # Statistical functions
â”‚   â”œâ”€â”€ visualization_helper.py  # Plotting functions
â”‚   â””â”€â”€ file_handler.py   # File operations
â”œâ”€â”€ sub_agents/           # Specialized agents
â”‚   â”œâ”€â”€ csv_analyzer/     # CSV analysis agent
â”‚   â”œâ”€â”€ research_analyzer/  # Research paper analysis
â”‚   â”œâ”€â”€ statistical_analyzer/  # Statistical testing
â”‚   â””â”€â”€ ml_analyzer/      # ML recommendations
â”œâ”€â”€ agent.py              # Main orchestrator
â””â”€â”€ prompt.py             # Main agent prompt
```

### Architecture Restructuring (v2.0)

To improve modularity and encapsulation, the core agent logic has been refactored and moved into the API service structure:

1. **New `api/agent` Module**: The core agent components (`RAGSystem`, `AnalysisPipeline`, `ChatHandler`) and `shared_libraries` have been moved to `api/agent`.
2. **Self-Contained API**: The API no longer depends on the external `data_science_agent` package for its core functionality, making it more robust and easier to deploy.
3. **Updated Imports**: All API routes (`api/routes/`) now import directly from `api.agent` instead of `data_science_agent`.

New Structure:
```
api/
â”œâ”€â”€ agent/                 # Encapsulated Agent Logic
â”‚   â”œâ”€â”€ shared_libraries/  # Data processing utilities
â”‚   â”œâ”€â”€ analysis_pipeline.py
â”‚   â”œâ”€â”€ chat_handler.py
â”‚   â””â”€â”€ rag_system.py
â”œâ”€â”€ routes/               # API Endpoints
â”œâ”€â”€ crud/                 # Database Operations
â””â”€â”€ models/               # Database Models
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10 or higher
- Poetry (for dependency management)
- Google API Key for Gemini

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd copilot-data-science-ml-agent
```

2. **Install Poetry** (if not already installed)
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

3. **Install dependencies**
```bash
poetry install
```

4. **Set up environment variables**
```bash
cp .env.example .env
```

Edit `.env` and add your configuration:
```env
# Google AI Configuration
GOOGLE_API_KEY=your_google_api_key_here

# Database Configuration (SQLite for local development)
DATABASE_URL=sqlite:///./data_science_agent.db

# Agent Configuration
MODEL_NAME=gemini-2.0-flash-exp
TEMPERATURE=0.7
MAX_ITERATIONS=10

# File Upload Configuration
UPLOAD_DIR=./data/uploads
MAX_FILE_SIZE_MB=100

# Session Configuration
SESSION_TIMEOUT_MINUTES=60
```

### Quick Start

#### 1. Interactive CLI Mode

```bash
poetry run python -m data_science_agent.agent
```

This starts an interactive session where you can:
- Ask questions about data science and ML
- Upload files for analysis
- Get statistical insights
- Receive ML recommendations

Example interaction:
```
You: file:/path/to/your/data.csv
Agent: [Loads and analyzes the file]

You: Show me descriptive statistics for all numeric columns
Agent: [Provides comprehensive statistics]

You: Create a correlation heatmap
Agent: [Generates and saves visualization]

You: Recommend ML algorithms for classification
Agent: [Provides algorithm recommendations]
```

#### 2. Python API Usage

```python
from data_science_agent.agent import DataScienceAgent

# Initialize agent
agent = DataScienceAgent()

# Create a session
session_id = agent.create_session(user_id="your_user_id")

# Analyze data
response = agent.analyze(
    session_id=session_id,
    user_message="Analyze this CSV file",
    file_paths=["data/sample.csv"]
)

print(response)

# Get session history
history = agent.get_session_history(session_id)

# Get analysis results
analyses = agent.get_session_analyses(session_id)
```

#### 3. Using Individual Tools

```python
from data_science_agent.sub_agents.csv_analyzer.tools import (
    load_csv_file,
    get_column_statistics,
    create_visualization
)

# Load CSV
result = load_csv_file("data/sample.csv")
print(result)

# Get statistics
stats = get_column_statistics("data/sample.csv", columns=["age", "salary"])
print(stats)

# Create visualization
viz = create_visualization(
    "data/sample.csv",
    viz_type="correlation_heatmap",
    title="Feature Correlations"
)
print(viz)
```

## ğŸ“Š Example Use Cases

### 1. Comprehensive EDA with Automatic Insights (NEW!)

```python
# Get automatic insights, patterns, and recommendations
agent.analyze(
    session_id,
    "Perform comprehensive EDA on this dataset with automatic insights",
    file_paths=["data/patients.csv"]
)

# Output includes:
# - Data quality score (0-100)
# - Automatic insights across 6 categories
# - Pattern detection
# - Anomaly identification
# - Prioritized recommendations
# - Visualization suggestions
```

### 2. Evidence-Based Analysis (NEW!)

```python
# Find evidence for specific claims
agent.analyze(
    session_id,
    "Is there evidence that treatment duration is related to patient age?",
    file_paths=["data/clinical_trial.csv"]
)

# Output includes:
# - Statistical evidence (correlations, p-values)
# - Confidence level
# - Supporting visualizations
# - Conclusion
```

### 3. Generate Data Stories (NEW!)

```python
# Create narrative summaries
agent.analyze(
    session_id,
    "Create a data story from this sales dataset",
    file_paths=["data/sales.csv"]
)

# Output includes:
# - Narrative structure with chapters
# - Key findings in story format
# - Evidence supporting each claim
# - Recommendations as next steps
```

### 4. Pattern Detection (NEW!)

```python
# Detect patterns automatically
agent.analyze(
    session_id,
    "What patterns and trends can you detect in this data?",
    file_paths=["data/timeseries.csv"]
)

# Output includes:
# - Upward/downward trends
# - Dominant categories
# - Cyclical patterns
# - Anomalies
```

### 5. Traditional CSV Data Analysis

```python
# Load and analyze a dataset (traditional approach)
agent.analyze(
    session_id,
    "Load the file and provide descriptive statistics and correlations",
    file_paths=["data/patients.csv"]
)
```

### 2. Statistical Testing

```python
# Perform a t-test
agent.analyze(
    session_id,
    "Perform an independent t-test comparing treatment_group and control_group on the outcome variable"
)
```

### 3. Research Paper Analysis

```python
# Analyze a research paper
agent.analyze(
    session_id,
    "Analyze the research paper at papers/study.pdf and extract key findings, methodology, and statistical results"
)
```

### 4. ML Recommendations

```python
# Get ML algorithm recommendations
agent.analyze(
    session_id,
    "I have a dataset with 50,000 rows and want to predict customer churn. Recommend suitable ML algorithms"
)
```

## ğŸ”§ Configuration

### Database Options

**SQLite (Default - Easiest)**
```env
DATABASE_URL=sqlite:///./data_science_agent.db
```

**PostgreSQL (Production)**
```env
DATABASE_URL=postgresql://user:password@localhost:5432/data_science_agent
```

### Model Configuration

Change the model in your `.env` file:
```env
MODEL_NAME=gemini-2.0-flash-exp  # Fast and efficient
# or
MODEL_NAME=gemini-pro  # More capable
```

## ğŸ“ Project Structure

```
copilot-data-science-ml-agent/
â”œâ”€â”€ data_science_agent/        # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py              # Main orchestrator
â”‚   â”œâ”€â”€ prompt.py             # Main prompt
â”‚   â”œâ”€â”€ database/             # Database layer
â”‚   â”œâ”€â”€ shared_libraries/     # Utilities
â”‚   â””â”€â”€ sub_agents/           # Specialized agents
â”‚       â”œâ”€â”€ csv_analyzer/
â”‚       â”‚   â”œâ”€â”€ prompt.py
â”‚       â”‚   â”œâ”€â”€ tools/
â”‚       â”‚   â””â”€â”€ __init__.py
â”‚       â”œâ”€â”€ research_analyzer/
â”‚       â”œâ”€â”€ statistical_analyzer/
â”‚       â””â”€â”€ ml_analyzer/
â”œâ”€â”€ data/                      # Data directory
â”‚   â”œâ”€â”€ uploads/              # Uploaded files
â”‚   â””â”€â”€ visualizations/       # Generated plots
â”œâ”€â”€ tests/                     # Test files
â”œâ”€â”€ eval/                      # Evaluation scripts
â”œâ”€â”€ deployment/               # Deployment configs
â”œâ”€â”€ pyproject.toml            # Poetry configuration
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ§ª Testing

Run tests with:
```bash
poetry run pytest tests/
```

## ğŸ“ˆ Evaluation

The system can be evaluated using the eval framework:

```bash
cd eval/
python evaluate.py
```

## ğŸš¢ Deployment

### Local Deployment

```bash
poetry run python -m data_science_agent.agent
```

### Docker Deployment

```bash
# Build image
docker build -t data-science-agent .

# Run container
docker run -p 8000:8000 -e GOOGLE_API_KEY=your_key data-science-agent
```

### Vertex AI Deployment

See `deployment/README.md` for instructions on deploying to Google Cloud Vertex AI.

## ğŸ› ï¸ Development

### Adding New Tools

1. Create a new tool function in the appropriate sub-agent's `tools/` directory
2. Register the tool in the sub-agent's `tools/__init__.py`
3. Add the tool to the main agent's tool registry in `agent.py`

### Adding New Sub-Agents

1. Create a new directory under `sub_agents/`
2. Add `prompt.py`, `agent.py`, and `tools/` directory
3. Implement the sub-agent's tools
4. Register in the main orchestrator

## ğŸ“š Documentation

- **Tool Documentation**: Each tool has comprehensive docstrings
- **API Documentation**: See `docs/API.md`
- **Architecture Guide**: See `docs/ARCHITECTURE.md`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with [Google ADK](https://github.com/google/adk-samples)
- Uses Google Gemini AI models
- Inspired by the Google ADK data-science agent example

## ğŸ“§ Support

For issues and questions:
- Open an issue on GitHub
- Check existing documentation
- Review example use cases

## ğŸ”® Future Enhancements

- [ ] Add support for more file formats (Excel, JSON, XML)
- [ ] Implement advanced ML model training capabilities
- [ ] Add natural language to SQL for database querying
- [ ] Integrate with BigQuery for large-scale data analysis
- [ ] Add real-time data streaming analysis
- [ ] Implement collaborative filtering recommendations
- [ ] Add automated report generation
- [ ] Integrate with Jupyter notebooks
- [ ] Add voice interface support
- [ ] Implement multi-language support

---

**Version**: 0.1.0
**Last Updated**: November 2024
**Status**: Active Development
